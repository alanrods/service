{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value):\n",
    "    \"\"\" Extract a set of n-grams from a list of integers.\n",
    "    \n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    \n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_ngram(sequences, token_indice, ngram_range):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:] #Copy of n-sequence [1, 3, 4, 5]\n",
    "        for i in range(len(new_list) - ngram_range + 1): #c=|s|-q+1; c=4-2+1\n",
    "            for ngram_value in range(2, ngram_range + 1): #\n",
    "                ngram = tuple(new_list[i:i + ngram_value])#Get the n-gram\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ds(path):\n",
    "    vecfile = open(path,'r')\n",
    "    return pickle.load(vecfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lValue(d_items):\n",
    "    values = d_items.values()\n",
    "    values.sort()\n",
    "    return values[-1]\n",
    "\n",
    "def get_value(d_items, key):\n",
    "    value = d_items.get(unicode(key),False) \n",
    "    if value:\n",
    "        return value\n",
    "    else:\n",
    "        value = get_lValue(d_items) + 1\n",
    "        d_items[key] = value #Add the new key to the Dict\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29505\n"
     ]
    }
   ],
   "source": [
    "dict_50_50 = pickle.load(open(\"./data/ann/dicFile_50_50.p\",\"rb\"))\n",
    "last_item = get_lValue(dict_50_50)\n",
    "print last_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_arch():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, \n",
    "                        embedding_size,\n",
    "                        input_length=maxlen))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(128, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(tweet_sec, target, nFolds):\n",
    "    kFold = StratifiedKFold(n_splits=nFolds, shuffle=True)\n",
    "    scores = []\n",
    "    for train, test in kFold.split(tweet_sec,target):\n",
    "        model = build_arch()\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(tweet_sec[train],target[train],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_split=0.1,\n",
    "                  verbose=1)\n",
    "        score, acc = model.evaluate(tweet_sec[test], target[test],batch_size=batch_size)\n",
    "        scores.append(acc * 100)\n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training with the 100% of the data set\n",
    "def train_model(tweet_sec,target):\n",
    "    model = build_arch()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model.fit(tweet_sec, target,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000 #Vocabulario\n",
    "maxlen = 20 #Secuence size \n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_range = 2\n",
    "#max_features = 20000\n",
    "#maxlen = 400\n",
    "#batch_size = 32\n",
    "#embedding_dims = 50\n",
    "#epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tweet_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[268, 459, 146, 2, 470, 4493]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2264, 7, 9784, 3050]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[80, 172, 35, 1, 14, 39, 2265, 7, 9785, 2266, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[107, 41, 21, 471, 9, 659, 5, 216, 300, 5, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[562, 106, 29, 563, 29, 422]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                          tweet_sec\n",
       "0         1                      [268, 459, 146, 2, 470, 4493]\n",
       "1         1                              [2264, 7, 9784, 3050]\n",
       "2         0  [80, 172, 35, 1, 14, 39, 2265, 7, 9785, 2266, ...\n",
       "3         1  [107, 41, 21, 471, 9, 659, 5, 216, 300, 5, 7, ...\n",
       "4         1                       [562, 106, 29, 563, 29, 422]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "ds = load_ds('./data/ann/vectors_50_50.txt')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "print('Average sequence length: {}'.format(np.mean(list(map(len, ds['tweet_sec'])), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "122565\n",
      "Average sequence length: 23\n"
     ]
    }
   ],
   "source": [
    "x_supreme = []\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in ds['tweet_sec']:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i) #Get the n_grams\n",
    "            ngram_set.update(set_of_ngram) #All the n_grams of the corpus\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = last_item + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "    \n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "    print max_features\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_supreme = add_ngram(ds['tweet_sec'], token_indice, ngram_range)\n",
    "    print('Average sequence length: {}'.format(np.mean(list(map(len, x_supreme)), dtype=int)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (15306, 20))\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_supreme = sequence.pad_sequences(x_supreme, maxlen=maxlen)\n",
    "print('x_train shape:', x_supreme.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 12396 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12396/12396 [==============================] - 82s - loss: 0.5266 - acc: 0.7369 - val_loss: 0.3608 - val_acc: 0.8447\n",
      "Epoch 2/5\n",
      "12396/12396 [==============================] - 83s - loss: 0.1243 - acc: 0.9611 - val_loss: 0.3533 - val_acc: 0.8483\n",
      "Epoch 3/5\n",
      "12396/12396 [==============================] - 81s - loss: 0.0218 - acc: 0.9952 - val_loss: 0.3821 - val_acc: 0.8403\n",
      "Epoch 4/5\n",
      "12396/12396 [==============================] - 82s - loss: 0.0078 - acc: 0.9983 - val_loss: 0.4158 - val_acc: 0.8331\n",
      "Epoch 5/5\n",
      "12396/12396 [==============================] - 79s - loss: 0.0036 - acc: 0.9994 - val_loss: 0.4429 - val_acc: 0.8382\n",
      "  32/1532 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12396 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12396/12396 [==============================] - 79s - loss: 0.5306 - acc: 0.7313 - val_loss: 0.3668 - val_acc: 0.8396\n",
      "Epoch 2/5\n",
      "12396/12396 [==============================] - 82s - loss: 0.1302 - acc: 0.9573 - val_loss: 0.3541 - val_acc: 0.8440\n",
      "Epoch 3/5\n",
      "12396/12396 [==============================] - 82s - loss: 0.0226 - acc: 0.9955 - val_loss: 0.3883 - val_acc: 0.8411\n",
      "Epoch 4/5\n",
      "12396/12396 [==============================] - 81s - loss: 0.0077 - acc: 0.9986 - val_loss: 0.4132 - val_acc: 0.8411\n",
      "Epoch 5/5\n",
      "12396/12396 [==============================] - 82s - loss: 0.0042 - acc: 0.9990 - val_loss: 0.4363 - val_acc: 0.8425\n",
      "  32/1532 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12396 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12396/12396 [==============================] - 86s - loss: 0.5274 - acc: 0.7392 - val_loss: 0.3719 - val_acc: 0.8433\n",
      "Epoch 2/5\n",
      "12396/12396 [==============================] - 80s - loss: 0.1249 - acc: 0.9628 - val_loss: 0.3608 - val_acc: 0.8411\n",
      "Epoch 3/5\n",
      "12396/12396 [==============================] - 82s - loss: 0.0216 - acc: 0.9959 - val_loss: 0.3900 - val_acc: 0.8309\n",
      "Epoch 4/5\n",
      "12396/12396 [==============================] - 80s - loss: 0.0073 - acc: 0.9986 - val_loss: 0.4232 - val_acc: 0.8324\n",
      "Epoch 5/5\n",
      "12396/12396 [==============================] - 79s - loss: 0.0035 - acc: 0.9993 - val_loss: 0.4510 - val_acc: 0.8302\n",
      "  32/1532 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.5363 - acc: 0.7288 - val_loss: 0.3764 - val_acc: 0.8440\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 80s - loss: 0.1281 - acc: 0.9601 - val_loss: 0.3610 - val_acc: 0.8454\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 84s - loss: 0.0227 - acc: 0.9947 - val_loss: 0.4039 - val_acc: 0.8396\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 85s - loss: 0.0080 - acc: 0.9985 - val_loss: 0.4237 - val_acc: 0.8454\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 84s - loss: 0.0038 - acc: 0.9993 - val_loss: 0.4681 - val_acc: 0.8360\n",
      "  32/1530 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 83s - loss: 0.5328 - acc: 0.7343 - val_loss: 0.3811 - val_acc: 0.8280\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.1260 - acc: 0.9615 - val_loss: 0.3711 - val_acc: 0.8418\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.0221 - acc: 0.9948 - val_loss: 0.4111 - val_acc: 0.8374\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.0078 - acc: 0.9986 - val_loss: 0.4304 - val_acc: 0.8360\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 85s - loss: 0.0039 - acc: 0.9994 - val_loss: 0.4602 - val_acc: 0.8382\n",
      "  32/1530 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 82s - loss: 0.5274 - acc: 0.7425 - val_loss: 0.3679 - val_acc: 0.8512\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.1264 - acc: 0.9609 - val_loss: 0.3569 - val_acc: 0.8433\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 82s - loss: 0.0222 - acc: 0.9952 - val_loss: 0.3956 - val_acc: 0.8374\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 84s - loss: 0.0078 - acc: 0.9989 - val_loss: 0.4168 - val_acc: 0.8353\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.0038 - acc: 0.9996 - val_loss: 0.4583 - val_acc: 0.8345\n",
      "  32/1530 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 80s - loss: 0.5380 - acc: 0.7317 - val_loss: 0.3687 - val_acc: 0.8447\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.1302 - acc: 0.9601 - val_loss: 0.3626 - val_acc: 0.8396\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 80s - loss: 0.0229 - acc: 0.9948 - val_loss: 0.3958 - val_acc: 0.8374\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 80s - loss: 0.0078 - acc: 0.9983 - val_loss: 0.4229 - val_acc: 0.8374\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 80s - loss: 0.0037 - acc: 0.9994 - val_loss: 0.4595 - val_acc: 0.8345\n",
      "  32/1530 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.5349 - acc: 0.7242 - val_loss: 0.3700 - val_acc: 0.8425\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.1274 - acc: 0.9608 - val_loss: 0.3470 - val_acc: 0.8447\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.0215 - acc: 0.9956 - val_loss: 0.3822 - val_acc: 0.8469\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.0076 - acc: 0.9987 - val_loss: 0.4105 - val_acc: 0.8447\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.0037 - acc: 0.9995 - val_loss: 0.4464 - val_acc: 0.8403\n",
      "1530/1530 [==============================] - 0s     \n",
      "Build model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.5332 - acc: 0.7388 - val_loss: 0.3680 - val_acc: 0.8360\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 81s - loss: 0.1264 - acc: 0.9598 - val_loss: 0.3586 - val_acc: 0.8382\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 78s - loss: 0.0211 - acc: 0.9953 - val_loss: 0.3936 - val_acc: 0.8396\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 78s - loss: 0.0072 - acc: 0.9988 - val_loss: 0.4158 - val_acc: 0.8374\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 78s - loss: 0.0034 - acc: 0.9994 - val_loss: 0.4388 - val_acc: 0.8295\n",
      "  32/1530 [..............................] - ETA: 0sBuild model...\n",
      "Train on 12398 samples, validate on 1378 samples\n",
      "Epoch 1/5\n",
      "12398/12398 [==============================] - 78s - loss: 0.5518 - acc: 0.7291 - val_loss: 0.3770 - val_acc: 0.8382\n",
      "Epoch 2/5\n",
      "12398/12398 [==============================] - 79s - loss: 0.1357 - acc: 0.9585 - val_loss: 0.3616 - val_acc: 0.8425\n",
      "Epoch 3/5\n",
      "12398/12398 [==============================] - 88s - loss: 0.0241 - acc: 0.9940 - val_loss: 0.3975 - val_acc: 0.8374\n",
      "Epoch 4/5\n",
      "12398/12398 [==============================] - 83s - loss: 0.0078 - acc: 0.9985 - val_loss: 0.4525 - val_acc: 0.8316\n",
      "Epoch 5/5\n",
      "12398/12398 [==============================] - 78s - loss: 0.0039 - acc: 0.9993 - val_loss: 0.4540 - val_acc: 0.8396\n",
      "  32/1530 [..............................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "results = cross_val(x_supreme,ds['category'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 83.81 (+/- 0.84)\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc: %.2f (+/- %.2f)\" %(np.mean(results), np.std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1, acc: 82.44 \n",
      "Model 2, acc: 85.05 \n",
      "Model 3, acc: 83.88 \n",
      "Model 4, acc: 84.05 \n",
      "Model 5, acc: 83.27 \n",
      "Model 6, acc: 83.14 \n",
      "Model 7, acc: 83.14 \n",
      "Model 8, acc: 84.77 \n",
      "Model 9, acc: 83.40 \n",
      "Model 10, acc: 84.97 \n"
     ]
    }
   ],
   "source": [
    "for i in range (0,len(results)):\n",
    "    print \"Model %d, acc: %.2f \" %(i+1,results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/5\n",
      "15306/15306 [==============================] - 103s - loss: 0.5027 - acc: 0.7555   \n",
      "Epoch 2/5\n",
      "15306/15306 [==============================] - 107s - loss: 0.1117 - acc: 0.9639   \n",
      "Epoch 3/5\n",
      "15306/15306 [==============================] - 109s - loss: 0.0193 - acc: 0.9958   \n",
      "Epoch 4/5\n",
      "15306/15306 [==============================] - 109s - loss: 0.0063 - acc: 0.9988   \n",
      "Epoch 5/5\n",
      "15306/15306 [==============================] - 111s - loss: 0.0031 - acc: 0.9994   \n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(x_supreme,ds['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = trained_model.model.to_json()\n",
    "with open(\"./data/models/fast_text/fast_text.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "trained_model.model.save_weights(\"./data/models/fast_text/fast_text_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model.model.save(\"./data/models/fast_text/fast_text.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(irnoy-env)",
   "language": "python",
   "name": "web"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
